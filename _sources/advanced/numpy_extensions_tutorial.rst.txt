.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_advanced_numpy_extensions_tutorial.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation

.. code-block:: default


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**


.. code-block:: default


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)







**Example usage of the created layer:**


.. code-block:: default


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[ 2.4000,  2.3729,  3.5275,  8.8525,  1.0993],
            [ 4.5754, 10.8506, 10.1935,  2.8883, 10.0866],
            [ 5.6680,  8.6943, 10.3539, 10.9466, 14.3920],
            [ 3.1709,  5.9852,  3.3286,  7.7343,  8.6673],
            [ 4.1605,  6.8557,  7.4958,  4.8683,  3.7295],
            [ 3.1709, 13.9429,  2.7557,  6.3811,  8.6673],
            [ 5.6680,  5.5308, 15.3715,  3.8995, 14.3920],
            [ 4.5754,  8.4886,  4.6428,  1.9680, 10.0866]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[ 0.3067, -0.1854, -1.1646,  1.0728, -0.7987,  0.0703, -1.2001, -0.3415],
            [ 1.1446, -2.8300, -0.2806, -1.5862,  1.7396,  0.5591,  0.7194,  0.1651],
            [-0.4664, -0.2400,  0.7312, -1.7305,  0.7298,  0.9120,  1.1473,  0.7835],
            [-0.5123, -0.1702,  0.2912,  1.4444,  0.8245,  0.0734,  0.1437, -0.5345],
            [ 0.7531, -0.5009, -0.7121,  0.4205, -1.8921, -0.9741, -0.6759,  1.3486],
            [ 0.2914,  0.2804,  0.6051,  0.4368,  0.6589,  0.7257, -1.7548, -0.3749],
            [-1.1148,  0.6328,  1.1844, -1.2296, -0.5215,  0.7435,  0.8464, -1.2149],
            [-0.5967,  1.4025,  0.2948, -1.4022, -0.8471,  0.8435, -1.6241,  0.7496]],
           requires_grad=True)


Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient wrt the input and the gradient wrt the filter.


.. code-block:: default


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)








**Example usage:**


.. code-block:: default


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[-0.1116, -0.1075, -0.8303],
            [ 0.4665, -0.1905, -1.1289],
            [-1.0259, -0.7282,  0.2964]], requires_grad=True), Parameter containing:
    tensor([[-0.2549]], requires_grad=True)]
    Output from the convolution:  tensor([[ 5.0032, -2.1906, -0.7217, -0.7449,  0.8340,  0.5928,  0.7340,  0.6796],
            [-4.3724, -2.1996, -0.6919,  2.4446,  1.5267, -2.6647, -2.4303, -0.8592],
            [ 1.5063,  0.9929, -0.3007, -0.4390, -1.3407,  0.4784,  0.6177, -0.6953],
            [-0.6093,  0.7058,  0.2643, -0.8842, -0.7166,  2.2524, -0.1543, -1.4855],
            [ 3.2133,  0.8605, -0.3763,  1.7559,  1.2827,  0.0356,  0.8583, -0.1478],
            [ 2.9769,  1.0760, -0.4423,  2.0868, -0.2900, -1.7777, -1.0432, -0.0210],
            [ 2.3134,  3.0814, -0.4752, -3.2245, -0.5570,  1.7740, -0.2454,  1.9237],
            [-1.2251, -0.8496, -1.2042, -0.5004, -0.7802, -0.0638,  2.4319, -3.0665]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[ 0.0070, -0.1778, -0.0558, -1.2280,  0.4091,  0.4810, -1.3248,  0.6351,
             -0.0289,  0.8080],
            [ 0.0111,  0.8965, -0.0842, -1.2953,  2.3003,  1.4619, -1.9465,  1.5694,
              0.0997,  3.0559],
            [-0.1138, -1.7844, -0.2727,  2.7429,  0.2383, -0.3661, -0.4584, -0.0230,
             -0.3965,  2.7227],
            [ 0.4744,  0.8001,  1.5765,  5.3704,  0.6545,  0.8834,  0.9671, -0.8982,
             -0.7435, -0.0190],
            [-0.3183, -0.0721,  3.3548,  2.4852, -1.9048, -0.2549, -2.1355, -3.1993,
              0.0884,  0.6271],
            [ 0.2109,  2.9022, -1.2849, -0.2712,  3.1391, -0.2840, -3.9568, -1.7407,
              2.1073,  1.1021],
            [ 1.1176, -0.5098, -2.3618,  1.0692, -2.0508, -3.4044, -1.7767,  1.3712,
              3.0025,  2.0188],
            [-1.4384, -0.9039, -0.9005, -0.4657, -4.5065, -2.9392,  2.8006,  1.6761,
              1.0124,  2.3871],
            [-1.7086, -2.3074,  0.6404, -0.2702, -3.2086,  0.0470,  1.1791,  1.6688,
              1.4136,  0.2634],
            [ 0.1445,  1.2736, -1.7420, -3.2594,  0.5343,  0.6790, -0.1030,  0.8231,
              0.4730, -0.2068]])


**Check the gradients:**


.. code-block:: default


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Are the gradients correct:  True



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.824 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
