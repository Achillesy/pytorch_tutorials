.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_advanced_numpy_extensions_tutorial.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation

.. code-block:: default


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**


.. code-block:: default


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)







**Example usage of the created layer:**


.. code-block:: default


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[10.2558,  6.6720,  3.4202,  6.6435,  0.5693],
            [16.6854, 10.0491,  6.1648,  4.8484,  9.9525],
            [ 4.3375,  5.7846,  8.8582,  2.5902,  3.1029],
            [ 5.1152, 11.0541,  9.4514, 12.0280,  7.1940],
            [ 6.6017, 12.9740,  7.5404, 10.8805,  4.3795],
            [ 5.1152, 12.6616,  8.5392,  8.3667,  7.1940],
            [ 4.3375, 13.9884,  3.9965,  3.7982,  3.1029],
            [16.6854,  9.1311,  7.7064,  2.6791,  9.9525]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[ 0.7248,  0.2749,  0.1290, -0.6092,  0.1941,  0.3031, -0.4603,  1.2848],
            [ 0.4529,  1.2565, -1.0946, -0.7896, -0.4175, -1.0352,  2.4811, -0.0097],
            [ 1.3968,  1.1838,  0.0175, -1.0774, -0.7189, -0.5487, -1.2471,  0.0643],
            [-1.0954,  0.8214, -1.2241, -1.1535, -1.0278,  0.2987,  0.1368, -0.1497],
            [-1.5818, -1.5888, -2.4349,  0.9721,  0.5871,  0.5072, -0.4491, -1.8145],
            [-0.1894, -0.2365, -0.1878, -0.6702, -2.0457, -1.5065, -0.4078,  1.3324],
            [-2.0076, -0.2694,  1.4935, -0.4516, -0.5092,  0.5164, -0.5854, -1.7240],
            [ 0.9951, -0.4770, -0.1541,  0.9859,  1.3437, -0.6959,  2.4734,  0.1628]],
           requires_grad=True)


Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient wrt the input and the gradient wrt the filter.


.. code-block:: default


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)








**Example usage:**


.. code-block:: default


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[ 1.1423, -1.4150,  0.1780],
            [-0.1306,  1.6709,  0.0529],
            [ 0.6049, -0.0100,  1.8960]], requires_grad=True), Parameter containing:
    tensor([[-0.9649]], requires_grad=True)]
    Output from the convolution:  tensor([[-1.6508, -0.7260, -1.0322,  2.3525, -1.1209, -2.6597, -3.3786, -2.9081],
            [-0.8253, -1.4367, -0.5653, -2.2730, -3.5317,  5.0401, -0.9652, -3.8556],
            [-1.2963, -1.2571, -0.7912,  4.2619, -0.0805, -4.6547,  6.2339,  1.8858],
            [ 2.1675, -1.3991, -1.4016,  0.2214,  2.3829,  1.9622, -2.9328,  2.0428],
            [ 3.4803, -0.3826, -0.3254, -1.4885, -5.1671,  1.0722,  2.0695, -6.6933],
            [-2.9544,  2.1529,  1.3937,  2.5840, -2.8730, -2.3375, -3.5215,  5.3779],
            [-1.6480, -2.7318,  3.5484,  2.3139, -3.4153,  2.3996,  3.8475, -9.9441],
            [-3.7833, -3.7093, -1.7254, -4.3133,  3.2031, -0.6329, -3.2613,  5.1833]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[-0.1565,  0.0488, -1.1795,  5.2554, -5.8457, -0.1918,  0.6650, -0.1802,
              2.3597, -0.3332],
            [ 0.8237, -2.1870,  1.0690, -2.3260,  4.6702, -1.4246, -0.3767, -1.3931,
             -6.8788,  0.3905],
            [-0.5812, -0.2740,  0.6105, -0.5238, -2.2473,  3.7578, -4.8012, -2.4465,
              2.1979, -3.4989],
            [ 2.0797, -3.6375, -0.3080, -2.1065, -1.0006, -0.4088, -3.8200, -1.4441,
              2.8869,  4.9842],
            [-0.1686,  1.4130, -2.2311, -2.3066, -0.9787, -1.6264,  0.2102, -5.1742,
             -2.2835, -1.2296],
            [-1.0819,  2.4184,  3.6395, -2.9707,  1.2738, -1.7654,  1.5377,  1.9299,
             -2.8039, -2.2301],
            [ 0.8695, -3.9571,  0.8582,  1.4954,  0.9717,  1.2845,  0.5284,  3.7500,
             -0.5975, -1.3795],
            [ 0.6494, -2.4345,  0.6133, -0.2473, -2.8273,  1.9380,  0.2474, -1.5170,
              3.7736, -0.2927],
            [ 0.0839,  2.3069, -1.2902,  1.7437,  3.9883, -1.9025,  0.3319, -0.4220,
             -1.2162,  2.8606],
            [ 0.9104, -0.6833,  4.1472, -0.6422,  3.7314,  4.6295, -1.0429, -0.1726,
             -0.6746, -0.6119]])


**Check the gradients:**


.. code-block:: default


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Are the gradients correct:  True



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.822 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
